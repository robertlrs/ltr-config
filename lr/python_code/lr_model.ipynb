{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 1, '上海': 2, '东南': 3, '临安': 4, '临沧': 5, '丽水': 6, '丽江': 7, '义乌': 8, '云南': 9, '佛山': 10, '保定': 11, '内蒙古': 12, '凉山': 13, '北京': 14, '南京': 15, '南昌': 16, '南通': 17, '南阳': 18, '厦门': 19, '台州': 20, '台湾': 21, '吉林': 22, '嘉兴': 23, '四川': 24, '四日': 25, '国外': 26, '大理': 27, '天津': 28, '宁夏': 29, '宁波': 30, '安徽': 31, '宜兴': 32, '宣城': 33, '山东': 34, '山西': 35, '广东': 36, '广州': 37, '广西': 38, '成都': 39, '扬州': 40, '新疆': 41, '新馀': 42, '无锡': 43, '日本': 44, '景德镇': 45, '杭州': 46, '武夷山': 47, '武汉': 48, '江苏': 49, '江西': 50, '江门': 51, '河北': 52, '河南': 53, '泉州': 54, '浙江': 55, '海南': 56, '深圳': 57, '温州': 58, '湖北': 59, '湖南': 60, '湖州': 61, '甘肃': 62, '眉山': 63, '石家庄': 64, '福州': 65, '福建': 66, '绍兴': 67, '苏州': 68, '茶道': 69, '莆田': 70, '衢州': 71, '西双版纳': 72, '西安': 73, '西藏': 74, '贵州': 75, '辽宁': 76, '重庆': 77, '金华': 78, '陕西': 79, '雅安': 80, '青岛': 81, '青海': 82, '香港': 83, '黄山': 84, '黑龙江': 85, '龙泉': 86}\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "feature = {\"type\":\"uType\", \"cid\":\"categoryId\", \"pid\":\"parentCategoryId\",\n",
    "        \"ctr\":\"reItemCtr\",\"collect1\":\"collectInterval\", \"add1\":\"addcartInterval\", \"tag\":\"kind\",\n",
    "        \"uid\":\"hashUid\", \"province_city\":\"uCity_province\", \"tag_city\":\"kind_province\", \n",
    "        \"tag_grade\":\"kind_categoryGrade\", \"tag_uid\":\"kind_hashUid\", \"province\":\"uCity\",\n",
    "         \"sale\":\"sellcntInterval\",\"ctr_sale\":\"reItemCtr_sellcntInterval\",\"iftag\":\"preferenceKind\",\"ifcid\":\"preferenceCategory\"\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"lr_sample_train\").getOrCreate()\n",
    "    craftsman_city = spark.sql(\"SELECT new_city, row_number() OVER(ORDER BY new_city) as num from (SELECT DISTINCT new_city from songwt.city_normalize) t\").collect()\n",
    "    _dict = dict()\n",
    "    for row in craftsman_city:\n",
    "        _dict[row.new_city] = row.num\n",
    "    \n",
    "    definition = list()\n",
    "    for fn in codecs.open(\"best_model\", \"rb\"):\n",
    "        ln = fn.strip().decode(\"utf8\")\n",
    "        lnn = ln.split(\"\\t\")\n",
    "        name = lnn[0]\n",
    "        value = lnn[1]\n",
    "        weight = float(lnn[2])\n",
    "        \n",
    "        if weight == 0.0 or name == \"(INTERCEPT)\":\n",
    "            continue\n",
    "        if len(name.split(\"_\")) == 2:\n",
    "            new_name = feature[name]\n",
    "            lr_name = [new_name.split(\"_\")[0], new_name.split(\"_\")[1]]\n",
    "        \n",
    "            if new_name.find(\"province\") != -1 and value.split(\"_\")[1] != \"null\":\n",
    "                \n",
    "                province = _dict[value.split(\"_\")[1]]\n",
    "\n",
    "                lr_term = [value.split(\"_\")[0], province]\n",
    "                \n",
    "            else:\n",
    "                lr_term = [value.split(\"_\")[0], value.split(\"_\")[1]]\n",
    "        else:\n",
    "            lr_name = [feature[name]]\n",
    "            lr_term = [value]\n",
    "        definition.append({\"name\":lr_name, \"term\":lr_term, \"weight\":weight})\n",
    "    _finally = {\"model\":{\"name\":\"index_lr_model\",\"model\":{\"type\":\"model/logistic\", \"definition\":definition}}}\n",
    "    wf = open(\"lr_model.json\", \"w\")\n",
    "    json.dump(_finally, wf)\n",
    "    print(\"finished\")\n",
    "    \n",
    "def test():\n",
    "    spark = SparkSession.builder.appName(\"lr_sample_train\").getOrCreate()\n",
    "    _list = []\n",
    "    for ln in codecs.open(\"craftsman.csv\", \"rb\"):\n",
    "        ln = ln.strip().decode(\"utf8\")\n",
    "        lnn = ln.split(\",\")\n",
    "        old_city = lnn[0]\n",
    "        new_city = lnn[3]\n",
    "        _list.append((old_city, new_city))\n",
    "    df = spark.createDataFrame(_list, ['old_city', 'new_city'])\n",
    "    df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"songwt.city_normalize\")\n",
    "    print(\"finished\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    #test()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
